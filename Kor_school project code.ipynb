{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "data=pd.read_csv('data3.csv',header=None)\n",
    "\n",
    "df=data.drop([0,1],0)\n",
    "\n",
    "df[28].replace(0,np.nan) # 학생수 0을 na 로 변환\n",
    "df2=df.dropna(subset=[28]) #전체학생수 na는 제거\n",
    "df3=df2.drop([45],1) \n",
    "#df3.info()\n",
    "df3[df3[28].values!=0]\n",
    "data=df3.iloc[1:, 12:] #데이터 추출\n",
    "\n",
    "data.columns=['일반고','특성화고','과고','외고','특목고합','예체고','마이스터','자율사','자율공','자율형합','기타','총진학','취업','미진학',\n",
    "              '교사1인당주수업','교사1인당학생','전체학생수',\n",
    "              '체험예산지원','1인당체험예산지원','동아리수','동아리참여학생수','동아리1인당참여율','동아리예산지원','1인당동아리예산',\n",
    "              '도서관자료비','도서운영비예산','1인당도서관예산','대출자료수','1인당 대출자료수',\n",
    "             '교과교실제', '자율학교', '이동수업', '영어']\n",
    "\n",
    "data['코드']=df3[3]\n",
    "data['지역']=df3[8]\n",
    "\n",
    "data=data[data.교과교실제!='0'] #na 값 제거\n",
    "data=data[data.자율학교!='0']\n",
    "\n",
    "dt=data.iloc[:, 29:33]\n",
    "data_enc=pd.get_dummies(dt) #one hot encoding\n",
    "dt1=data.iloc[:, :29]\n",
    "dt2=data.iloc[:, 33:]\n",
    "hak=pd.concat([dt1,data_enc,dt2],axis=1)\n",
    "hak.전체학생수.apply(pd.to_numeric)\n",
    "pd.to_numeric(hak['전체학생수'])\n",
    "hak[['일반고','특성화고','과고','외고','특목고합','예체고','마이스터','자율사','자율공','자율형합','기타','총진학','취업','미진학','교사1인당주수업','교사1인당학생','전체학생수',\n",
    "              '체험예산지원','1인당체험예산지원','동아리수','동아리참여학생수','동아리1인당참여율','동아리예산지원','1인당동아리예산',\n",
    "              '도서관자료비','도서운영비예산','1인당도서관예산','대출자료수','1인당 대출자료수']]=hak[['일반고','특성화고','과고','외고','특목고합','예체고','마이스터','자율사','자율공','자율형합','기타','총진학','취업','미진학','교사1인당주수업','교사1인당학생','전체학생수',\n",
    "              '체험예산지원','1인당체험예산지원','동아리수','동아리참여학생수','동아리1인당참여율','동아리예산지원','1인당동아리예산',\n",
    "              '도서관자료비','도서운영비예산','1인당도서관예산','대출자료수','1인당 대출자료수']].astype(float)\n",
    "\n",
    "hak['score']=3*hak['특목고합']+1.5*hak['자율형합']+hak['일반고']+0.8*hak['마이스터']+0.2*hak['특성화고']\n",
    "hak=hak[hak.score!=0] #이상치 제거\n",
    "hak=hak[hak.예체고<=50]#이상치 제거\n",
    "\n",
    "\n",
    "inputs=hak[['특목고합','자율형합','일반고','마이스터','특성화고','예체고']]\n",
    "x=inputs.values #data type (df->array)\n",
    "kmeans = KMeans(n_clusters=4, random_state=0).fit(x)\n",
    "lab=kmeans.labels_\n",
    "df_lab = pd.DataFrame(lab, columns=['label'])\n",
    "\n",
    "hak_re=hak.reset_index(drop=True) #인덱스 재정의 후 통합\n",
    "hak2=pd.concat([hak_re,df_lab],axis=1) \n",
    "\n",
    "hak2=hak2.dropna() \n",
    "\n",
    "\n",
    "targetX=hak2.iloc[:,[14,15,18,21,23,26,28,30,32,34,36]]\n",
    "targetY=hak2.label\n",
    "data_train,data_test,label_train,label_test=train_test_split(targetX,targetY,test_size=.3,random_state=0)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### 데이터 전처리 - 데이터 표준화 작업\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "\n",
    "\n",
    "\n",
    "sc = RobustScaler()\n",
    "x_train_std = sc.fit_transform(data_train)\n",
    "x_test_std = sc.transform(data_test)\n",
    "\n",
    "\n",
    "#log scale\n",
    "#x_train_log=np.log(x_train)\n",
    "\n",
    "\n",
    "\n",
    "#label 값을 카테고리화\n",
    "y_train=label_train.astype('category')\n",
    "y_test=label_test.astype('category')\n",
    "\n",
    "#모형 시각화\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "plt.rc('font',family='Malgun Gothic')\n",
    "\n",
    "\n",
    "\n",
    "# if hak2['score']<=0, hak2['label']=3 을 4로 변화\n",
    "# 조건에 맞게 값 변화\n",
    "#hak2.loc[hak2['score'] <= 30, ['label']] = 4\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hak2.sort_values('예체고',ascending=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_train.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_test.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hak2['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hak2.corr() #상관관계"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "nb_classes = 4\n",
    "a = targetY\n",
    "targets = np.array([a]).reshape(-1)\n",
    "one_hot_targets = np.eye(nb_classes)[targets]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_hot_targets #라벨값을 one hot encording"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one = pd.DataFrame(one_hot_targets)\n",
    "# hak2=pd.concat([hak2,one],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs=hak[['특목고합','자율형합','일반고','마이스터','특성화고','예체고']]\n",
    "x=inputs.values #data type (df->array)\n",
    "kmeans = KMeans(n_clusters=5, random_state=0).fit(x)\n",
    "print(kmeans)\n",
    "print(kmeans.cluster_centers_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs=hak[['특목고합','자율형합','일반고','마이스터','특성화고','예체고']]\n",
    "x=inputs.values #data type (df->array)\n",
    "kmeans = KMeans(n_clusters=4, random_state=0).fit(x)\n",
    "print(kmeans)\n",
    "print(kmeans.cluster_centers_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs=hak[['특목고합','자율형합','일반고','마이스터','특성화고','예체고']]\n",
    "x=inputs.values #data type (df->array)\n",
    "y=kmeans.fit_predict(inputs)\n",
    "kmeans = KMeans(n_clusters=4, random_state=0).fit(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hierarchical Clustering and Dendrogram (TARGET VAR 분류)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns \n",
    "\n",
    "sns.set(color_codes=True)\n",
    "\n",
    "g = sns.clustermap(x,cmap=\"mako\", robust=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some setting for this notebook to actually show the graphs inline\n",
    "# you probably won't need this\n",
    "%matplotlib inline\n",
    "np.set_printoptions(precision=5, suppress=True)  # suppress scientific float notation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=x\n",
    "print (X.shape)  \n",
    "plt.scatter(X[:,1], X[:,4])\n",
    "plt.show()\n",
    "# 1:자율형고 ,4:특성화고"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate the linkage matrix\n",
    "# agglomerative hierarchical clustering (bottom up) - Ward Linkage Method\n",
    "\n",
    "Z = linkage(X, 'ward')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.cluster.hierarchy import cophenet\n",
    "from scipy.spatial.distance import pdist\n",
    "\n",
    "c, coph_dists = cophenet(Z, pdist(X))\n",
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Z [1000: 1200 ]\n",
    "#결과 배열의 ach 행의 형식은 [idx1, idx2, dist, sample_count] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate full dendrogram\n",
    "plt.figure(figsize=(25, 10))\n",
    "plt.title('Hierarchical Clustering Dendrogram')\n",
    "plt.xlabel('school index')\n",
    "plt.ylabel('distance')\n",
    "dendrogram(\n",
    "    Z,\n",
    "    leaf_rotation=90.,  # rotates the x axis labels\n",
    "    leaf_font_size=8.,  # font size for the x axis labels\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title('Hierarchical Clustering Dendrogram (truncated)')\n",
    "plt.xlabel('school index')\n",
    "plt.ylabel('distance')\n",
    "dendrogram(\n",
    "    Z,\n",
    "    truncate_mode='lastp',  # show only the last p merged clusters\n",
    "    p=8,  # show only the last p merged clusters\n",
    "    show_leaf_counts=False,  # otherwise numbers in brackets are counts\n",
    "    leaf_rotation=90.,\n",
    "    leaf_font_size=12.,\n",
    "    show_contracted=True,  # to get a distribution impression in truncated branches\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title('Hierarchical Clustering Dendrogram (truncated)')\n",
    "plt.xlabel('school index or (cluster size)')\n",
    "plt.ylabel('distance')\n",
    "dendrogram(\n",
    "    Z,\n",
    "    truncate_mode='lastp',  # show only the last p merged clusters\n",
    "    p=8,  # show only the last p merged clusters\n",
    "    leaf_rotation=90.,\n",
    "    leaf_font_size=12.,\n",
    "    show_contracted=True,  # to get a distribution impression in truncated branches\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fancy_dendrogram(*args, **kwargs):\n",
    "    max_d = kwargs.pop('max_d', None)\n",
    "    if max_d and 'color_threshold' not in kwargs:\n",
    "        kwargs['color_threshold'] = max_d\n",
    "    annotate_above = kwargs.pop('annotate_above', 0)\n",
    "\n",
    "    ddata = dendrogram(*args, **kwargs)\n",
    "\n",
    "    if not kwargs.get('no_plot', False):\n",
    "        plt.title('Hierarchical Clustering Dendrogram (truncated)')\n",
    "        plt.xlabel('sample index or (cluster size)')\n",
    "        plt.ylabel('distance')\n",
    "        for i, d, c in zip(ddata['icoord'], ddata['dcoord'], ddata['color_list']):\n",
    "            x = 0.5 * sum(i[1:3])\n",
    "            y = d[1]\n",
    "            if y > annotate_above:\n",
    "                plt.plot(x, y, 'o', c=c)\n",
    "                plt.annotate(\"%.3g\" % y, (x, y), xytext=(0, -5),\n",
    "                             textcoords='offset points',\n",
    "                             va='top', ha='center')\n",
    "        if max_d:\n",
    "            plt.axhline(y=max_d, c='k')\n",
    "    return ddata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fancy_dendrogram(\n",
    "    Z,\n",
    "    truncate_mode='lastp',\n",
    "    p=8,\n",
    "    leaf_rotation=90.,\n",
    "    leaf_font_size=12.,\n",
    "    show_contracted=True,\n",
    "    annotate_above=10,  # useful in small plots so annotations don't overlap\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set cut-off to 50\n",
    "max_d = 400  # max_d as in max_distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we got 5 clusting if we give max_d=400\n",
    "\n",
    "fancy_dendrogram(\n",
    "    Z,\n",
    "    truncate_mode='lastp',\n",
    "    p=12,\n",
    "    leaf_rotation=90.,\n",
    "    leaf_font_size=12.,\n",
    "    show_contracted=True,\n",
    "    annotate_above=10,\n",
    "    max_d=max_d,  # plot a horizontal cut-off line\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we got 4 clusting if we give max_d=500\n",
    "\n",
    "fancy_dendrogram(\n",
    "    Z,\n",
    "    truncate_mode='lastp',\n",
    "    p=12,\n",
    "    leaf_rotation=90.,\n",
    "    leaf_font_size=12.,\n",
    "    show_contracted=True,\n",
    "    annotate_above=10,\n",
    "    max_d=500,\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.cluster.hierarchy import inconsistent\n",
    "\n",
    "depth = 10\n",
    "incons = inconsistent(Z, depth)\n",
    "incons[-10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "depth = 3\n",
    "incons = inconsistent(Z, depth)\n",
    "incons[-10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#It tries to find the clustering step where the acceleration of distance growth \n",
    "#is the biggest (the \"strongest elbow\" of the blue line graph below, which is the highest value of the green graph below)\n",
    "\n",
    "last = Z[-10:, 2]\n",
    "last_rev = last[::-1]\n",
    "idxs = np.arange(1, len(last) + 1)\n",
    "plt.plot(idxs, last_rev) \n",
    "\n",
    "acceleration = np.diff(last, 2)  # 2nd derivative of the distances\n",
    "acceleration_rev = acceleration[::-1]\n",
    "plt.plot(idxs[:-2] + 1, acceleration_rev) \n",
    "plt.show()\n",
    "k = acceleration_rev.argmax() + 2  # if idx 0 is the max of this we want 2 clusters\n",
    "print (\"clusters:\", k)\n",
    "\n",
    "# 2 다음에 5나 4가 가장 높다\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#other way to find number of clust\n",
    "#fcluster function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if Knowing max_d\n",
    "\n",
    "from scipy.cluster.hierarchy import fcluster\n",
    "max_d = 5\n",
    "fcluster(Z, max_d, criterion='distance')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#if Knowing k\n",
    "\n",
    "k=4\n",
    "clusters=fcluster(Z, k, criterion='maxclust')\n",
    "clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#if want Using the Inconsistency Method\n",
    "\n",
    "from scipy.cluster.hierarchy import fcluster\n",
    "fcluster(Z, 4, depth=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 8))\n",
    "plt.scatter(X[:,1], X[:,4], c=clusters)  # plot points with cluster dependent colors  \n",
    "plt.show()\n",
    "\n",
    "# 1:자율형고 ,4: 특성화고\n",
    "\n",
    "# k=4\n",
    "# clusters=fcluster(Z, k, criterion='maxclust')\n",
    "# clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hierarchical Clustering and Dendrogram (설명변수로 분류)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "targetX\n",
    "sc = RobustScaler()\n",
    "target_std = sc.fit_transform(targetX)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x= target_std\n",
    "\n",
    "import seaborn as sns \n",
    "\n",
    "sns.set(color_codes=True)\n",
    "g = sns.clustermap(x,cmap=\"mako\", robust=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some setting for this notebook to actually show the graphs inline\n",
    "# you probably won't need this\n",
    "%matplotlib inline\n",
    "np.set_printoptions(precision=5, suppress=True)  # suppress scientific float notation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=x\n",
    "print (X.shape)  \n",
    "plt.scatter(X[:,0], X[:,1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate the linkage matrix\n",
    "Z = linkage(X, 'ward')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.cluster.hierarchy import cophenet\n",
    "from scipy.spatial.distance import pdist\n",
    "\n",
    "c, coph_dists = cophenet(Z, pdist(X))\n",
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Z [1000: 1200 ]\n",
    "#결과 배열의 ach 행의 형식은 [idx1, idx2, dist, sample_count] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate full dendrogram\n",
    "plt.figure(figsize=(25, 10))\n",
    "plt.title('Hierarchical Clustering Dendrogram')\n",
    "plt.xlabel('school index')\n",
    "plt.ylabel('distance')\n",
    "dendrogram(\n",
    "    Z,\n",
    "    leaf_rotation=90.,  # rotates the x axis labels\n",
    "    leaf_font_size=8.,  # font size for the x axis labels\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title('Hierarchical Clustering Dendrogram (truncated)')\n",
    "plt.xlabel('school index')\n",
    "plt.ylabel('distance')\n",
    "dendrogram(\n",
    "    Z,\n",
    "    truncate_mode='lastp',  # show only the last p merged clusters\n",
    "    p=4,  # show only the last p merged clusters\n",
    "    show_leaf_counts=False,  # otherwise numbers in brackets are counts\n",
    "    leaf_rotation=90.,\n",
    "    leaf_font_size=12.,\n",
    "    show_contracted=True,  # to get a distribution impression in truncated branches\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title('Hierarchical Clustering Dendrogram (truncated)')\n",
    "plt.xlabel('school index or (cluster size)')\n",
    "plt.ylabel('distance')\n",
    "dendrogram(\n",
    "    Z,\n",
    "    truncate_mode='lastp',  # show only the last p merged clusters\n",
    "    p=4,  # show only the last p merged clusters\n",
    "    leaf_rotation=90.,\n",
    "    leaf_font_size=12.,\n",
    "    show_contracted=True,  # to get a distribution impression in truncated branches\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "last = Z[-10:, 2]\n",
    "last_rev = last[::-1]\n",
    "idxs = np.arange(1, len(last) + 1)\n",
    "plt.plot(idxs, last_rev) \n",
    "\n",
    "acceleration = np.diff(last, 2)  # 2nd derivative of the distances\n",
    "acceleration_rev = acceleration[::-1]\n",
    "plt.plot(idxs[:-2] + 1, acceleration_rev) \n",
    "plt.show()\n",
    "k = acceleration_rev.argmax() + 2  # if idx 0 is the max of this we want 2 clusters\n",
    "print (\"clusters:\", k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# kmeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs=hak[['특목고합','자율형합','일반고','마이스터','특성화고','예체고']]\n",
    "x=inputs.values #data type (df->array)\n",
    "y=kmeans.fit_predict(inputs)\n",
    "kmeans = KMeans(n_clusters=4, random_state=0).fit(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2차원 그래프라서 차별성이 강한 일반고,특성화고 두개만 선택.\n",
    "\n",
    "plt.scatter(x[y==0,2],x[y==0,4],c='lightgreen', marker='s',label='clu1')\n",
    "plt.scatter(x[y==1,2],x[y==1,4],c='orange', marker='o',label='clu2')\n",
    "plt.scatter(x[y==2,2],x[y==2,4],c='lightblue', marker='v',label='clu3')\n",
    "plt.scatter(x[y==3,2],x[y==3,4],c='b', marker='^',label='clu4')\n",
    "\n",
    "plt.scatter(kmeans.cluster_centers_[:,2],kmeans.cluster_centers_[:,4],c='red', marker='*',label='center')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#자율형고 와 특성화고\n",
    "\n",
    "plt.scatter(x[y==0,1],x[y==0,4],c='lightgreen', marker='s',label='clu1')\n",
    "plt.scatter(x[y==1,1],x[y==1,4],c='orange', marker='o',label='clu2')\n",
    "plt.scatter(x[y==2,1],x[y==2,4],c='lightblue', marker='v',label='clu3')\n",
    "plt.scatter(x[y==3,1],x[y==3,4],c='b', marker='^',label='clu4')\n",
    "\n",
    "plt.scatter(kmeans.cluster_centers_[:,1],kmeans.cluster_centers_[:,4],c='red', marker='*',label='center')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SSE(오차제곱합) 최소되도록 최적 k값 찾기 \n",
    "def elbow(x):\n",
    "    sse=[]\n",
    "    for i in range(1,11):\n",
    "        y=KMeans(n_clusters=i, init='k-means++', random_state=0)\n",
    "        y.fit(x)\n",
    "        sse.append(y.inertia_)\n",
    "\n",
    "    plt.plot(range(1,11),sse,marker='o')\n",
    "    plt.xlabel('cluster number')\n",
    "    plt.ylabel('SSE')\n",
    "    plt.show()\n",
    "\n",
    "elbow(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#정확한 답(클러스터의 갯수 및 소속)을 검사위해 clustring의 품질을 정량적으로 계산\n",
    "#Silhouette Coefficient\n",
    "#실루엣 계수가 1에 가까울수록 클러스터 n 최적화\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_samples, silhouette_score\n",
    "import matplotlib.cm as cm\n",
    "\n",
    "\n",
    "\n",
    "X=x\n",
    "range_n_clusters = [2, 3, 4, 5, 6, 7]\n",
    "\n",
    "for n_clusters in range_n_clusters:\n",
    "    # Create a subplot with 1 row and 2 columns\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2)\n",
    "    fig.set_size_inches(18, 7)\n",
    "\n",
    "    # The 1st subplot is the silhouette plot\n",
    "    # The silhouette coefficient can range from -1, 1 but in this example all\n",
    "    # lie within [-0.1, 1]\n",
    "    ax1.set_xlim([-0.1, 1])\n",
    "    # The (n_clusters+1)*10 is for inserting blank space between silhouette\n",
    "    # plots of individual clusters, to demarcate them clearly.\n",
    "    ax1.set_ylim([0, len(X) + (n_clusters + 1) * 100])\n",
    "\n",
    "    # Initialize the clusterer with n_clusters value and a random generator\n",
    "    # seed of 10 for reproducibility.\n",
    "    clusterer = KMeans(n_clusters=n_clusters, random_state=10)\n",
    "    cluster_labels = clusterer.fit_predict(X)\n",
    "\n",
    "    # The silhouette_score gives the average value for all the samples.\n",
    "    # This gives a perspective into the density and separation of the formed\n",
    "    # clusters\n",
    "    silhouette_avg = silhouette_score(X, cluster_labels)\n",
    "    print(\"For n_clusters =\", n_clusters,\n",
    "          \"The average silhouette_score is :\", silhouette_avg)\n",
    "\n",
    "    # Compute the silhouette scores for each sample\n",
    "    sample_silhouette_values = silhouette_samples(X, cluster_labels)\n",
    "\n",
    "    y_lower = 10\n",
    "    \n",
    "    for i in range(n_clusters):\n",
    "        \n",
    "        # Aggregate the silhouette scores for samples belonging to\n",
    "        # cluster i, and sort them\n",
    "        ith_cluster_silhouette_values = \\\n",
    "            sample_silhouette_values[cluster_labels == i]\n",
    "\n",
    "        ith_cluster_silhouette_values.sort()\n",
    "\n",
    "        size_cluster_i = ith_cluster_silhouette_values.shape[0]\n",
    "        y_upper = y_lower + size_cluster_i\n",
    "\n",
    "        #color = cm.spectral(float(i) / n_clusters)\n",
    "        cmap = cm.get_cmap(\"Spectral\")\n",
    "        color = cmap(float(i) / n_clusters)\n",
    "        \n",
    "        ax1.fill_betweenx(np.arange(y_lower, y_upper),\n",
    "                          0, ith_cluster_silhouette_values,\n",
    "                          facecolor=color, edgecolor=color, alpha=0.7)\n",
    "\n",
    "        # Label the silhouette plots with their cluster numbers at the middle\n",
    "        ax1.text(-0.05, y_lower + 0.5 * size_cluster_i, str(i+1))\n",
    "\n",
    "        # Compute the new y_lower for next plot\n",
    "        y_lower = y_upper + 10  # 10 for the 0 samples\n",
    "\n",
    "    ax1.set_title(\"The silhouette plot for the various clusters.\")\n",
    "    ax1.set_xlabel(\"The silhouette coefficient values\")\n",
    "    ax1.set_ylabel(\"Cluster label\")\n",
    "\n",
    "    # The vertical line for average silhouette score of all the values\n",
    "    ax1.axvline(x=silhouette_avg, color=\"red\", linestyle=\"--\")\n",
    "\n",
    "    ax1.set_yticks([])  # Clear the yaxis labels / ticks\n",
    "    ax1.set_xticks([-0.1, 0, 0.2, 0.4, 0.6, 0.8, 1])\n",
    "\n",
    "    # 2nd Plot showing the actual clusters formed\n",
    "    #colors = cm.spectral(cluster_labels.astype(float) / n_clusters)\n",
    "    cmap = cm.get_cmap(\"Spectral\")\n",
    "    colors = cmap(cluster_labels.astype(float) / n_clusters)    \n",
    "    \n",
    "    ax2.scatter(X[:, 2], X[:, 4], marker='.', s=30, lw=0, alpha=0.7,\n",
    "                c=colors)\n",
    "\n",
    "    # Labeling the clusters\n",
    "    centers = clusterer.cluster_centers_\n",
    "    # Draw white circles at cluster centers\n",
    "    ax2.scatter(centers[:, 2], centers[:, 4],\n",
    "                marker='o', c=\"yellow\", alpha=1, s=200)\n",
    "\n",
    "    for i, c in enumerate(centers):\n",
    "        r=i+1\n",
    "        ax2.scatter(c[2], c[4], marker='$%d$' % r, alpha=1, s=50)\n",
    "\n",
    "    ax2.set_title(\"The visualization of the clustered data.\")\n",
    "    ax2.set_xlabel(\"Feature space for the 1st feature\")\n",
    "    ax2.set_ylabel(\"Feature space for the 2nd feature\")\n",
    "\n",
    "    plt.suptitle((\"Silhouette analysis for KMeans clustering on sample data \"\n",
    "                  \"with n_clusters = %d\" % n_clusters),\n",
    "                 fontsize=14, fontweight='bold')\n",
    "\n",
    "    plt.show()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=x\n",
    "range_n_clusters = [2, 3, 4, 5, 6, 7, 8]\n",
    "\n",
    "for n_clusters in range_n_clusters:\n",
    "    # Create a subplot with 1 row and 2 columns\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2)\n",
    "    fig.set_size_inches(18, 7)\n",
    "\n",
    "    # The 1st subplot is the silhouette plot\n",
    "    # The silhouette coefficient can range from -1, 1 but in this example all\n",
    "    # lie within [-0.1, 1]\n",
    "    ax1.set_xlim([-0.1, 1])\n",
    "    # The (n_clusters+1)*10 is for inserting blank space between silhouette\n",
    "    # plots of individual clusters, to demarcate them clearly.\n",
    "    ax1.set_ylim([0, len(X) + (n_clusters + 1) * 100])\n",
    "\n",
    "    # Initialize the clusterer with n_clusters value and a random generator\n",
    "    # seed of 10 for reproducibility.\n",
    "    clusterer = KMeans(n_clusters=n_clusters, random_state=10)\n",
    "    cluster_labels = clusterer.fit_predict(X)\n",
    "\n",
    "    # The silhouette_score gives the average value for all the samples.\n",
    "    # This gives a perspective into the density and separation of the formed\n",
    "    # clusters\n",
    "    silhouette_avg = silhouette_score(X, cluster_labels)\n",
    "    print(\"For n_clusters =\", n_clusters,\n",
    "          \"The average silhouette_score is :\", silhouette_avg)\n",
    "\n",
    "    # Compute the silhouette scores for each sample\n",
    "    sample_silhouette_values = silhouette_samples(X, cluster_labels)\n",
    "\n",
    "    y_lower = 10\n",
    "    \n",
    "    for i in range(n_clusters):\n",
    "        \n",
    "        # Aggregate the silhouette scores for samples belonging to\n",
    "        # cluster i, and sort them\n",
    "        ith_cluster_silhouette_values = \\\n",
    "            sample_silhouette_values[cluster_labels == i]\n",
    "\n",
    "        ith_cluster_silhouette_values.sort()\n",
    "\n",
    "        size_cluster_i = ith_cluster_silhouette_values.shape[0]\n",
    "        y_upper = y_lower + size_cluster_i\n",
    "\n",
    "        #color = cm.spectral(float(i) / n_clusters)\n",
    "        cmap = cm.get_cmap(\"Spectral\")\n",
    "        color = cmap(float(i) / n_clusters)\n",
    "        \n",
    "        ax1.fill_betweenx(np.arange(y_lower, y_upper),\n",
    "                          0, ith_cluster_silhouette_values,\n",
    "                          facecolor=color, edgecolor=color, alpha=0.7)\n",
    "\n",
    "        # Label the silhouette plots with their cluster numbers at the middle\n",
    "        ax1.text(-0.05, y_lower + 0.5 * size_cluster_i, str(i+1))\n",
    "\n",
    "        # Compute the new y_lower for next plot\n",
    "        y_lower = y_upper + 10  # 10 for the 0 samples\n",
    "\n",
    "    ax1.set_title(\"The silhouette plot for the various clusters.\")\n",
    "    ax1.set_xlabel(\"The silhouette coefficient values\")\n",
    "    ax1.set_ylabel(\"Cluster label\")\n",
    "\n",
    "    # The vertical line for average silhouette score of all the values\n",
    "    ax1.axvline(x=silhouette_avg, color=\"red\", linestyle=\"--\")\n",
    "\n",
    "    ax1.set_yticks([])  # Clear the yaxis labels / ticks\n",
    "    ax1.set_xticks([-0.1, 0, 0.2, 0.4, 0.6, 0.8, 1])\n",
    "\n",
    "    # 2nd Plot showing the actual clusters formed\n",
    "    #colors = cm.spectral(cluster_labels.astype(float) / n_clusters)\n",
    "    cmap = cm.get_cmap(\"Spectral\")\n",
    "    colors = cmap(cluster_labels.astype(float) / n_clusters)\n",
    "    \n",
    "    ax2.scatter(X[:, 1], X[:, 4], marker='.', s=30, lw=0, alpha=0.7,\n",
    "                c=colors)\n",
    "\n",
    "    # Labeling the clusters\n",
    "    centers = clusterer.cluster_centers_\n",
    "    # Draw white circles at cluster centers\n",
    "    ax2.scatter(centers[:, 1], centers[:, 4],\n",
    "                marker='o', c=\"yellow\", alpha=1, s=200)\n",
    "\n",
    "    for i, c in enumerate(centers):\n",
    "        r=i+1\n",
    "        ax2.scatter(c[1], c[4], marker='$%d$' % r, alpha=1, s=50)\n",
    "\n",
    "    ax2.set_title(\"The visualization of the clustered data.\")\n",
    "    ax2.set_xlabel(\"Feature space for the 1st feature\")\n",
    "    ax2.set_ylabel(\"Feature space for the 2nd feature\")\n",
    "\n",
    "    plt.suptitle((\"Silhouette analysis for KMeans clustering on sample data \"\n",
    "                  \"with n_clusters = %d\" % n_clusters),\n",
    "                 fontsize=14, fontweight='bold')\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 라벨링\n",
    "plt.scatter(x[y==0,2],x[y==0,4],c='lightgreen', marker='s',label='진학 목적 하')\n",
    "plt.scatter(x[y==1,2],x[y==1,4],c='orange', marker='o',label='진학 목적 중')\n",
    "plt.scatter(x[y==2,2],x[y==2,4],c='lightblue', marker='v',label='취업 목적')\n",
    "plt.scatter(x[y==3,2],x[y==3,4],c='b', marker='^',label='진학 목적 상')\n",
    "plt.rc('font',family='Malgun Gothic')\n",
    "plt.scatter(kmeans.cluster_centers_[:,2],kmeans.cluster_centers_[:,4],c='red', marker='*',label='center')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hak['score']=3*hak['특목고합']+1.5*hak['자율형합']+hak['일반고']+0.8*hak['마이스터']+0.2*hak['특성화고']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hak.boxplot('score')\n",
    "plt.ylim(0,200)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hak['score'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3d 시각화\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "var=hak.iloc[:,[0,1,4,5,6,9]]\n",
    "targetY=hak.score\n",
    "\n",
    "data_train,data_test,label_train,label_test=train_test_split(var,targetY,test_size=.3) #var:x변수,targetY:y변수\n",
    "\n",
    "kmeans = KMeans(n_clusters=4, random_state=0).fit(data_train[['일반고','특성화고','자율형합']])\n",
    "print(kmeans)\n",
    "print('center:',kmeans.cluster_centers_)\n",
    "\n",
    "# for i in range(kmeans.labels_.size):\n",
    "\t# print(kmeans.labels_[i])\n",
    "\n",
    "pred=kmeans.predict(data_test[['일반고','특성화고','자율형합']])\n",
    "# for i in range(pred.size):\n",
    "# \tprint(pred[i])\n",
    "\n",
    "X = data_train[['일반고','특성화고','자율형합']].as_matrix().astype(int)\n",
    "y = data_test\n",
    "estimators = [('k_means_4', KMeans(n_clusters=4)),\n",
    "              ('k_means_5', KMeans(n_clusters=5))]\n",
    "\n",
    "print('예측:', estimators)\n",
    "fignum = 1\n",
    "titles = ['4 clusters', '5 clusters']\n",
    "for name, est in estimators:\n",
    "    fig = plt.figure(fignum, figsize=(8, 7))\n",
    "    ax = Axes3D(fig, rect=[0, 0, .95, 1], elev=48, azim=134)\n",
    "    est.fit(X)\n",
    "    labels = est.labels_\n",
    "    print(labels[0:3])\n",
    "    ax.scatter(X[:, 0], X[:, 1], X[:, 2],\n",
    "           c=labels.astype(np.float))\n",
    "\n",
    "    ax.set_xlabel('일반고')\n",
    "    ax.set_ylabel('특성화고')\n",
    "    ax.set_zlabel('자율형고')\n",
    "    ax.set_title(titles[fignum - 1])\n",
    "    fignum = fignum + 1\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# target 변수간 상관관계도\n",
    "import seaborn as sns\n",
    "import scipy.stats as stat\n",
    "\n",
    "hak_df1=hak[['특성화고','특목고합','예체고','마이스터','자율형합','기타','취업','미진학']]\n",
    "sns.pairplot(hak_df1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2.설명변수간 상관관계\n",
    "#산점행렬도 (Scatter Plot matrix)=행렬산점도\n",
    "import seaborn as sns\n",
    "import scipy.stats as stat\n",
    "\n",
    "hak_df=hak[['score','교사1인당주수업','교사1인당학생','1인당체험예산지원','동아리1인당참여율','1인당동아리예산','1인당도서관예산','1인당 대출자료수']]\n",
    "sns.pairplot(hak_df)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ty=hak.score\n",
    "fx1=hak.교사1인당학생\n",
    "print('pearsonr: ',stat.pearsonr(ty,fx1))\n",
    "print('spearmanr: ',stat.spearmanr(ty,fx1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ty=hak.score\n",
    "fx2=hak['1인당 대출자료수']\n",
    "print('pearsonr: ',stat.pearsonr(ty,fx2))\n",
    "print('spearmanr: ',stat.spearmanr(ty,fx2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ty=hak.score\n",
    "fx3=hak['동아리1인당참여율']\n",
    "print('pearsonr: ',stat.pearsonr(ty,fx3))\n",
    "print('spearmanr: ',stat.spearmanr(ty,fx3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.jointplot(x='교사1인당학생', y='교사1인당주수업', data=hak, size=5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PCA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "data=pd.read_csv('data3.csv',header=None)\n",
    "\n",
    "df=data.drop([0,1],0)\n",
    "\n",
    "df[28].replace(0,np.nan) # 학생수 0을 na 로 변환\n",
    "df2=df.dropna(subset=[28]) #전체학생수 na는 제거\n",
    "df3=df2.drop([45],1) \n",
    "#df3.info()\n",
    "df3[df3[28].values!=0]\n",
    "data=df3.iloc[1:, 12:] #데이터 추출\n",
    "\n",
    "data.columns=['일반고','특성화고','과고','외고','특목고합','예체고','마이스터','자율사','자율공','자율형합','기타','총진학','취업','미진학',\n",
    "              '교사1인당주수업','교사1인당학생','전체학생수',\n",
    "              '체험예산지원','1인당체험예산지원','동아리수','동아리참여학생수','동아리1인당참여율','동아리예산지원','1인당동아리예산',\n",
    "              '도서관자료비','도서운영비예산','1인당도서관예산','대출자료수','1인당 대출자료수',\n",
    "             '교과교실제', '자율학교', '이동수업', '영어']\n",
    "\n",
    "data['코드']=df3[3]\n",
    "data['지역']=df3[8]\n",
    "\n",
    "data=data[data.교과교실제!='0'] #na 값 제거\n",
    "data=data[data.자율학교!='0']\n",
    "dt=data.iloc[:, 29:33]\n",
    "data_enc=pd.get_dummies(dt) #one hot encoding\n",
    "dt1=data.iloc[:, :29]\n",
    "dt2=data.iloc[:, 33:]\n",
    "hak=pd.concat([dt1,data_enc,dt2],axis=1)\n",
    "hak.전체학생수.apply(pd.to_numeric)\n",
    "pd.to_numeric(hak['전체학생수'])\n",
    "hak[['일반고','특성화고','과고','외고','특목고합','예체고','마이스터','자율사','자율공','자율형합','기타','총진학','취업','미진학','교사1인당주수업','교사1인당학생','전체학생수',\n",
    "              '체험예산지원','1인당체험예산지원','동아리수','동아리참여학생수','동아리1인당참여율','동아리예산지원','1인당동아리예산',\n",
    "              '도서관자료비','도서운영비예산','1인당도서관예산','대출자료수','1인당 대출자료수']]=hak[['일반고','특성화고','과고','외고','특목고합','예체고','마이스터','자율사','자율공','자율형합','기타','총진학','취업','미진학','교사1인당주수업','교사1인당학생','전체학생수',\n",
    "              '체험예산지원','1인당체험예산지원','동아리수','동아리참여학생수','동아리1인당참여율','동아리예산지원','1인당동아리예산',\n",
    "              '도서관자료비','도서운영비예산','1인당도서관예산','대출자료수','1인당 대출자료수']].astype(float)\n",
    "\n",
    "hak['score']=3*hak['특목고합']+1.5*hak['자율형합']+hak['일반고']+0.8*hak['마이스터']+0.2*hak['특성화고']\n",
    "hak=hak[hak.score!=0] #이상치 제거\n",
    "hak=hak[hak.예체고<=50]#이상치 제거\n",
    "\n",
    "\n",
    "inputs=hak[['특목고합','자율형합','일반고','마이스터','특성화고','예체고']]\n",
    "x=inputs.values #data type (df->array)\n",
    "kmeans = KMeans(n_clusters=4, random_state=0).fit(x)\n",
    "lab=kmeans.labels_\n",
    "df_lab = pd.DataFrame(lab, columns=['label'])\n",
    "\n",
    "hak_re=hak.reset_index(drop=True) #인덱스 재정의 후 통합\n",
    "hak2=pd.concat([hak_re,df_lab],axis=1) \n",
    "\n",
    "hak2=hak2.dropna() \n",
    "\n",
    "\n",
    "targetX=hak2.iloc[:,[14,15,18,21,23,26,28,30,32,34,36]]\n",
    "targetY=hak2.label\n",
    "data_train,data_test,label_train,label_test=train_test_split(targetX,targetY,test_size=.3,random_state=0)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### 데이터 전처리 - 데이터 표준화 작업\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "\n",
    "\n",
    "\n",
    "sc = RobustScaler()\n",
    "x_train_std = sc.fit_transform(data_train)\n",
    "x_test_std = sc.transform(data_test)\n",
    "\n",
    "\n",
    "#log scale\n",
    "#x_train_log=np.log(x_train)\n",
    "\n",
    "\n",
    "\n",
    "#label 값을 카테고리화\n",
    "y_train=label_train.astype('category')\n",
    "y_test=label_test.astype('category')\n",
    "\n",
    "#모형 시각화\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "plt.rc('font',family='Malgun Gothic')\n",
    "\n",
    "\n",
    "\n",
    "# if hak2['score']<=0, hak2['label']=3 을 4로 변화\n",
    "# 조건에 맞게 값 변화\n",
    "#hak2.loc[hak2['score'] <= 30, ['label']] = 4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "X =x_train_std\n",
    "pca = PCA(n_components=2)\n",
    "pca.fit(X)\n",
    "PCA(copy=True, iterated_power='auto', n_components=2, random_state=None,\n",
    "  svd_solver='auto', tol=0.0, whiten=False)\n",
    "print(pca.explained_variance_ratio_)  \n",
    "\n",
    "print(pca.singular_values_)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1=data_train.iloc[:,[3,6]] #학생변수\n",
    "x1_std=sc.fit_transform(x1)\n",
    "\n",
    "x2=data_train.iloc[:,[0,1]] #교사변수\n",
    "x2_std=sc.fit_transform(x1)\n",
    "\n",
    "x3=data_train.iloc[:,[2,4,5,7,8,9,10]] #학교변수\n",
    "x3_std=sc.fit_transform(x1)\n",
    "\n",
    "x_train_std = sc.fit_transform(data_train)  #변수 전체\n",
    "\n",
    "x4=data_train.iloc[:,[0,5,6]] #3그룹의 대표변수\n",
    "x4_std=sc.fit_transform(x4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "\n",
    "#대표변수 시각화\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "plt.rc('font',family='Malgun Gothic')\n",
    "\n",
    "\n",
    "X =x4_std\n",
    "#X=x_train_std[:,0]\n",
    "y =y_train\n",
    "target_names = ['진학 중','취업','진학 상','진학 하']\n",
    "\n",
    "# 0:진학 중\n",
    "# 1:취업\n",
    "# 2:진학 상\n",
    "# 3:진학 하\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "X_r = pca.fit(X).transform(X)\n",
    "\n",
    "lda = LinearDiscriminantAnalysis(n_components=2)\n",
    "X_r2 = lda.fit(X, y).transform(X)\n",
    "\n",
    "# Percentage of variance explained for each components\n",
    "print('explained variance ratio (first two components): %s'\n",
    "      % str(pca.explained_variance_ratio_))\n",
    "\n",
    "plt.figure()\n",
    "colors = ['pink','red' , 'blue'] # '취업','진학 상'의 구분이 잘 보이기 위해 '진학 하' 의 색을 제거\n",
    "lw = 0.05\n",
    "\n",
    "for color, i, target_name in zip(colors, np.unique(y_train), target_names):\n",
    "    plt.scatter(X_r[y == i, 0], X_r[y == i, 1], color=color, alpha=.8, lw=lw,\n",
    "                label=target_name)\n",
    "plt.legend(loc='best', shadow=False, scatterpoints=1)\n",
    "plt.title('PCA of school dataset')\n",
    "\n",
    "plt.figure()\n",
    "for color, i, target_name in zip(colors, np.unique(y_train), target_names):\n",
    "    plt.scatter(X_r2[y == i, 0], X_r2[y == i, 1], alpha=.8, color=color,lw=lw,\n",
    "                label=target_name)\n",
    "plt.legend(loc='best', shadow=False, scatterpoints=1)\n",
    "plt.title('LDA of school dataset')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#전체변수 시각화\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "plt.rc('font',family='Malgun Gothic')\n",
    "\n",
    "\n",
    "X =x_train_std\n",
    "#X=x_train_std[:,0]\n",
    "y =y_train\n",
    "target_names = ['진학 중','취업','진학 상','진학 하']\n",
    "\n",
    "# 0:진학 중\n",
    "# 1:취업\n",
    "# 2:진학 상\n",
    "# 3:진학 하\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "X_r = pca.fit(X).transform(X)\n",
    "\n",
    "lda = LinearDiscriminantAnalysis(n_components=2)\n",
    "X_r2 = lda.fit(X, y).transform(X)\n",
    "\n",
    "# Percentage of variance explained for each components\n",
    "print('explained variance ratio (first two components): %s'\n",
    "      % str(pca.explained_variance_ratio_))\n",
    "\n",
    "plt.figure()\n",
    "colors = ['pink', 'red', 'blue'] # '취업','진학 상'의 구분이 잘 보이기 위해 '진학 하' 의 색을 제거\n",
    "lw = 0.05\n",
    "\n",
    "for color, i, target_name in zip(colors, np.unique(y_train), target_names):\n",
    "    plt.scatter(X_r[y == i, 0], X_r[y == i, 1], color=color, alpha=.8, lw=lw,\n",
    "                label=target_name)\n",
    "plt.legend(loc='best', shadow=False, scatterpoints=1)\n",
    "plt.title('PCA of school dataset')\n",
    "\n",
    "plt.figure()\n",
    "for color, i, target_name in zip(colors, np.unique(y_train), target_names):\n",
    "    plt.scatter(X_r2[y == i, 0], X_r2[y == i, 1], alpha=.8, color=color,lw=lw,\n",
    "                label=target_name)\n",
    "plt.legend(loc='best', shadow=False, scatterpoints=1)\n",
    "plt.title('LDA of school dataset')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 공분산 행렬을 이용한 Eigendecomposition\n",
    "import numpy as np\n",
    "\n",
    "cov_mat = np.cov(x_train_std.T) # 공분산 행렬을 생성해주는 함수\n",
    "# T는 Matrix의 T를 의미. 함수에 맞는 파라미터로 쓰기 위해 행렬을 돌려줌\n",
    "\n",
    "eigen_vals, eigen_vecs = np.linalg.eig(cov_mat)\n",
    "\n",
    "print('\\nEigenvalues \\n%s' % eigen_vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 에이겐벨류의 설명 분산 비율\n",
    "tot = sum(eigen_vals)\n",
    "var_exp = [(i / tot) for i in sorted(eigen_vals, reverse=True)]\n",
    "# 에이겐벨류 / 에이겐벨류의 합 을 각각 구한다. 나온 각각의 값은 아이겐벨류의 설명 분산 비율이다.\n",
    "# 즉, 어떤 에이겐벨류가 가장 설명력이 높은지를 비율로 나타내기 위한 것이다.\n",
    "\n",
    "cum_var_exp = np.cumsum(var_exp) # 누적 합을 계산해주는 함수. -> 누적 백분위로 표현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 에이겐벨류의 영향력을 그래프로 시각화\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "plt.bar(range(1, 12), var_exp, alpha=0.5, align='center',label='individual explained variance')\n",
    "plt.step(range(1, 12), cum_var_exp, where='mid',label='cumulative explained variance')\n",
    "plt.ylabel('Explained variance ratio')\n",
    "plt.xlabel('Principal components')\n",
    "plt.legend(loc='best')\n",
    "plt.tight_layout()\n",
    "# plt.savefig('./figures/pca1.png', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 에이겐 쌍을 이용하여 투영행렬 생성\n",
    "eigen_pairs = [(np.abs(eigen_vals[i]), eigen_vecs[:,i]) for i in range(len(eigen_vals))]\n",
    "# 에이겐 쌍 생성 -> 투플 자료형\n",
    "\n",
    "eigen_pairs.sort(reverse=True) # 내림차순으로 정렬\n",
    "\n",
    "w = np.hstack((eigen_pairs[0][1][:, np.newaxis],\n",
    "               eigen_pairs[1][1][:, np.newaxis]))\n",
    "# 투영행렬 W : 변수를 2차원으로 축소시키는 투영행렬.\n",
    "# eigen_pairs의 0,1 번째만 -> 2개의 에이겐 쌍으로만 차원축소를 하겠다는 것.\n",
    "# hstack -> 행의 수가 같은 두 개 이상의 배열을 옆으로 연결하여, 열의 수가 늘어난 np배열을 만든다.\n",
    "# 1차원 배열끼리는 hstack 되지 않으므로 [:, np.newaxis]을 추가함.\n",
    "\n",
    "print('Matrix W:\\n', w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 투영행렬로 피처 압축\n",
    "x_train_std[0].dot(w) # x_train_std[0] 행렬과 W 행렬의 곱(내적연산)\n",
    "\n",
    "x_train_pca = x_train_std.dot(w) # 피처를 투영행렬에 곱한 값 -> 피처 축소된 결과\n",
    "\n",
    "\n",
    "### 변환된 데이터를 그래프로 시각화\n",
    "colors = ['r', 'b', 'g','y']\n",
    "markers = ['s', 'x', 'o','^']\n",
    "\n",
    "for l, c, m in zip(np.unique(y_train), colors, markers):\n",
    "    plt.scatter(x_train_pca[y_train==l, 0], \n",
    "                x_train_pca[y_train==l, 1], \n",
    "                c=c, label=l, marker=m)\n",
    "\n",
    "plt.xlabel('PC 1')\n",
    "plt.ylabel('PC 2')\n",
    "plt.legend(loc='upper right')\n",
    "plt.tight_layout()\n",
    "# plt.savefig('./figures/pca2.png', dpi=300)\n",
    "plt.show()\n",
    "\n",
    "# 0:진학 하\n",
    "# 1:진학 중\n",
    "# 2:취업\n",
    "# 3:진학 상"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pca(n=3)\n",
    "from statsmodels.multivariate.pca  import  PCA  as pca\n",
    "pc=pca( x_train_std, ncomp=3, standardize=True, normalize=True, gls=False, weights=None, method='svd')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pc.factors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pc.plot_scree()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pc.plot_rsquare()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pc의 적정 갯수?\n",
    "# import sys; sys.path.append('~~/lib')\n",
    "from mylib import plot_decision_regions\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Training logistic regression classifier using the first 2 principal components.\n",
    "\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "X_train_pca = pca.fit_transform(x_train_std)\n",
    "X_test_pca = pca.transform(x_test_std)\n",
    "\n",
    "lr = LogisticRegression()\n",
    "lr = lr.fit(X_train_pca, y_train)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "plot_decision_regions(X_train_pca, y_train, classifier=lr)\n",
    "plt.xlabel('PC 1')\n",
    "plt.ylabel('PC 2')\n",
    "plt.legend(loc='lower right')\n",
    "plt.tight_layout()\n",
    "# plt.savefig('images/05_04.png', dpi=300)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "plot_decision_regions(X_test_pca, y_test, classifier=lr)\n",
    "plt.xlabel('PC 1')\n",
    "plt.ylabel('PC 2')\n",
    "plt.legend(loc='lower right')\n",
    "plt.tight_layout()\n",
    "# plt.savefig('images/05_05.png', dpi=300)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "X_train_pca = pca.fit_transform(x_train_std)\n",
    "pca.explained_variance_ratio_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pca n=3\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "X =x_train_std\n",
    "pca = PCA(n_components=3)\n",
    "pca.fit(X)\n",
    "PCA(copy=True, iterated_power='auto', n_components=3, random_state=None,\n",
    "  svd_solver='auto', tol=0.0, whiten=False)\n",
    "print(pca.explained_variance_ratio_)  \n",
    "\n",
    "print(pca.singular_values_)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DT (Classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn import tree\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score,confusion_matrix,classification_report,accuracy_score\n",
    "import graphviz\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "model=DecisionTreeClassifier( random_state = 0 )\n",
    "clf= tree.DecisionTreeClassifier(random_state = 0) #model\n",
    "clf=clf.fit(x_train_std,y_train) #fittedModel 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "cross_val_score(clf, targetX, targetY, cv=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "cross_val_score(model, x_train_std, y_train, cv=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.metrics import confusion_matrix\n",
    "y_pred = cross_val_predict(clf,x_test_std, y_test,cv=10)\n",
    "conf_mat = confusion_matrix(y_test,y_pred)\n",
    "\n",
    "conf_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict=clf.predict(x_test_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classification_report(y_test,predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accurateRate=accuracy_score(y_test,predict)\n",
    "print('정확도: ',accurateRate)\n",
    "print('오류:',1-accurateRate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix(y_test,predict) \n",
    "# 0:진학 하\n",
    "# 1:진학 중\n",
    "# 2:취업\n",
    "# 3:진학 상"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 파라미터 탐색 결과 가장 좋은 모델과 validation set 에 대한 정확도"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = RobustScaler()\n",
    "targetX_std = sc.fit_transform(targetX)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn import metrics\n",
    "\n",
    "depth_set=[3,4,5,6,7,8,9,10]\n",
    "dt_models=[]\n",
    "accuracy_set=[]\n",
    "\n",
    "\n",
    "for depth in depth_set:\n",
    "    model=DecisionTreeClassifier(max_depth=depth, random_state=0)\n",
    "    model.fit(x_train_std,y_train)\n",
    "    predict=model.predict(x_test_std)\n",
    "    accuracy=metrics.accuracy_score(y_test,predict)\n",
    "    #predict=model.predict(targetX_std)\n",
    "    #accuracy=metrics.accuracy_score(targetY,predict)\n",
    "    \n",
    "    dt_models.append(model)\n",
    "    accuracy_set.append(accuracy)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(accuracy_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 파라미터 탐색 결과 가장 좋은 모델과 validation set 에 대한 정확도\n",
    "max_value=max(accuracy_set)\n",
    "max_index=accuracy_set.index(max_value)\n",
    "print(max_index)\n",
    "print(max_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 가장 좋은 모델\n",
    "dt_models[max_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#가장 좋은 모델을 가져와 Test_set 에 대한 예측 성능 평가\n",
    "y_test_hat=dt_models[max_index].predict(x_test_std)\n",
    "print('정확도:',metrics.accuracy_score(y_test,y_test_hat))\n",
    "print(metrics.confusion_matrix(y_test,y_test_hat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf2=dt_models[max_index]\n",
    "clf2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import export_graphviz\n",
    "import matplotlib.pyplot\n",
    "export_graphviz(clf2,out_file='hak_DC.dot',impurity=False,filled=True)\n",
    "\n",
    "\n",
    "#트리모형 그래프\n",
    "import graphviz\n",
    "\n",
    "with open('hak_DC.dot') as f:\n",
    "    dot_graph=f.read()\n",
    "graphviz.Source(dot_graph)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#트리모형 시각화\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "plt.rc('font',family='Malgun Gothic')\n",
    "\n",
    "def plot_feature_importance(model):\n",
    "    n_features=data_train.shape[1] #열\n",
    "    plt.barh(range(n_features),model.feature_importances_,align='center')\n",
    "    \n",
    "    plt.xlabel('특성중요도')\n",
    "    plt.ylabel('특성')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict=clf2.predict(x_test_std)\n",
    "names = data_train.columns.values\n",
    "print(names)\n",
    "importances = clf2.feature_importances_\n",
    "sorted_importances = np.argsort(importances)\n",
    "print(sorted_importances)\n",
    "\n",
    "print(\"변수 중요도\")\n",
    "print(sorted(zip(map(lambda x: round(x, 4), clf2.feature_importances_), names), reverse=True))\n",
    "padding = np.arange(len(names)) + 0.5\n",
    "plt.barh(padding, importances[sorted_importances], align='center')\n",
    "# plt.barh(padding, importances[sorted_importances])\n",
    "plt.yticks(padding, names[sorted_importances])\n",
    "plt.xlabel(\"Relative Importance\")\n",
    "plt.title(\"Feature Importance\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('feature importances : ', importances)\n",
    "indices = np.argsort(importances)[::-1]\n",
    "\n",
    "print('indices : ', indices)\n",
    "arrTrain = data_train.as_matrix().astype(int)\n",
    "featuresTrain = arrTrain[:,[0,1,2,3,4,5,6,7,8,9,10]]\n",
    "\n",
    "for f in range(featuresTrain.shape[1]):\n",
    "    print(\"%d. feature %d (%f)\" % (f + 1, indices[f], importances[indices[f]]))\n",
    "\n",
    "\n",
    "f, ax = plt.subplots(figsize=(11, 9))\n",
    "plt.title(\"Feature ranking\", fontsize = 20)\n",
    "plt.bar(range(featuresTrain.shape[1]), importances[indices],\n",
    "    color=\"b\", \n",
    "    align=\"center\")\n",
    "plt.xticks(range(featuresTrain.shape[1]), indices)\n",
    "plt.xlim([-1, featuresTrain.shape[1]])\n",
    "plt.ylabel(\"importance\", fontsize = 18)\n",
    "plt.xlabel(\"index of the feature\", fontsize = 18)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DT (Reggresion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hak2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "data=pd.read_csv('data3.csv',header=None)\n",
    "\n",
    "df=data.drop([0,1],0)\n",
    "\n",
    "df[28].replace(0,np.nan) # 학생수 0을 na 로 변환\n",
    "df2=df.dropna(subset=[28]) #전체학생수 na는 제거\n",
    "df3=df2.drop([45],1) \n",
    "#df3.info()\n",
    "df3[df3[28].values!=0]\n",
    "data=df3.iloc[1:, 12:] #데이터 추출\n",
    "\n",
    "data.columns=['일반고','특성화고','과고','외고','특목고합','예체고','마이스터','자율사','자율공','자율형합','기타','총진학','취업'\n",
    "              ,'미진학',\n",
    "              '교사1인당주수업','교사1인당학생','전체학생수',\n",
    "              '체험예산지원','1인당체험예산지원','동아리수','동아리참여학생수','동아리1인당참여율','동아리예산지원','1인당동아리예산',\n",
    "              '도서관자료비','도서운영비예산','1인당도서관예산','대출자료수','1인당 대출자료수',\n",
    "             '교과교실제', '자율학교', '이동수업', '영어']\n",
    "\n",
    "data['코드']=df3[3]\n",
    "data['지역']=df3[8]\n",
    "\n",
    "data=data[data.교과교실제!='0'] #na 값 제거\n",
    "data=data[data.자율학교!='0']\n",
    "dt=data.iloc[:, 29:33]\n",
    "data_enc=pd.get_dummies(dt) #one hot encoding\n",
    "dt1=data.iloc[:, :29]\n",
    "dt2=data.iloc[:, 33:]\n",
    "hak=pd.concat([dt1,data_enc,dt2],axis=1)\n",
    "hak.전체학생수.apply(pd.to_numeric)\n",
    "pd.to_numeric(hak['전체학생수'])\n",
    "hak[['일반고','특성화고','과고','외고','특목고합','예체고','마이스터','자율사','자율공','자율형합','기타','총진학','취업','미진학','교사1인당주수업','교사1인당학생','전체학생수',\n",
    "              '체험예산지원','1인당체험예산지원','동아리수','동아리참여학생수','동아리1인당참여율','동아리예산지원','1인당동아리예산',\n",
    "              '도서관자료비','도서운영비예산','1인당도서관예산','대출자료수','1인당 대출자료수']]=hak[['일반고','특성화고','과고','외고','특목고합','예체고','마이스터','자율사','자율공','자율형합','기타','총진학','취업','미진학','교사1인당주수업','교사1인당학생','전체학생수',\n",
    "              '체험예산지원','1인당체험예산지원','동아리수','동아리참여학생수','동아리1인당참여율','동아리예산지원','1인당동아리예산',\n",
    "              '도서관자료비','도서운영비예산','1인당도서관예산','대출자료수','1인당 대출자료수']].astype(float)\n",
    "\n",
    "hak['score']=3*hak['특목고합']+1.5*hak['자율형합']+hak['일반고']+0.8*hak['마이스터']+0.2*hak['특성화고']\n",
    "hak=hak[hak.score!=0] #이상치 제거\n",
    "hak=hak[hak.예체고<=50]#이상치 제거\n",
    "\n",
    "\n",
    "inputs=hak[['특목고합','자율형합','일반고','마이스터','특성화고','예체고']]\n",
    "x=inputs.values #data type (df->array)\n",
    "kmeans = KMeans(n_clusters=4, random_state=0).fit(x)\n",
    "lab=kmeans.labels_\n",
    "df_lab = pd.DataFrame(lab, columns=['label'])\n",
    "\n",
    "hak_re=hak.reset_index(drop=True) #인덱스 재정의 후 통합\n",
    "hak2=pd.concat([hak_re,df_lab],axis=1) \n",
    "\n",
    "hak2=hak2.dropna() \n",
    "\n",
    "\n",
    "targetX=hak2.iloc[:,[14,15,18,21,23,26,28,30,32,34,36]]\n",
    "targetY=hak2.score  #regression 용도 연속형 변수\n",
    "data_train,data_test,label_train,label_test=train_test_split(targetX,targetY,test_size=.3,random_state=0)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### 데이터 전처리 - 데이터 표준화 작업\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "\n",
    "\n",
    "\n",
    "sc = RobustScaler()\n",
    "x_train_std = sc.fit_transform(data_train)\n",
    "x_test_std = sc.transform(data_test)\n",
    "\n",
    "\n",
    "#log scale\n",
    "#x_train_log=np.log(x_train)\n",
    "\n",
    "\n",
    "\n",
    "#label 값을 카테고리화\n",
    "# y_train=label_train.astype('category')\n",
    "# y_test=label_test.astype('category')\n",
    "\n",
    "#모형 시각화\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "plt.rc('font',family='Malgun Gothic')\n",
    "\n",
    "\n",
    "\n",
    "# if hak2['score']<=0, hak2['label']=3 을 4로 변화\n",
    "# 조건에 맞게 값 변화\n",
    "#hak2.loc[hak2['score'] <= 30, ['label']] = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn import tree\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score,confusion_matrix,classification_report,accuracy_score\n",
    "import graphviz\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "model=DecisionTreeRegressor( random_state = 0 )\n",
    "clf= tree.DecisionTreeRegressor(random_state = 0) #model\n",
    "clf=clf.fit(x_train_std,label_train) #fittedModel 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import export_graphviz\n",
    "import matplotlib.pyplot\n",
    "export_graphviz(clf,out_file='hak_DR.dot',impurity=False,filled=True)\n",
    "\n",
    "\n",
    "#트리모형 그래프\n",
    "import graphviz\n",
    "\n",
    "with open('hak_DR.dot') as f:\n",
    "    dot_graph=f.read()\n",
    "graphviz.Source(dot_graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#트리모형 시각화\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "plt.rc('font',family='Malgun Gothic')\n",
    "\n",
    "def plot_feature_importance(model):\n",
    "    n_features=data_train.shape[1] #열\n",
    "    plt.barh(range(n_features),model.feature_importances_,align='center')\n",
    "    \n",
    "    plt.xlabel('특성중요도')\n",
    "    plt.ylabel('특성')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict=clf.predict(x_test_std)\n",
    "names = data_train.columns.values\n",
    "print(names)\n",
    "importances = clf.feature_importances_\n",
    "sorted_importances = np.argsort(importances)\n",
    "print(sorted_importances)\n",
    "\n",
    "print(\"변수 중요도\")\n",
    "print(sorted(zip(map(lambda x: round(x, 4), clf.feature_importances_), names), reverse=True))\n",
    "padding = np.arange(len(names)) + 0.5\n",
    "plt.barh(padding, importances[sorted_importances], align='center')\n",
    "# plt.barh(padding, importances[sorted_importances])\n",
    "plt.yticks(padding, names[sorted_importances])\n",
    "plt.xlabel(\"Relative Importance\")\n",
    "plt.title(\"Feature Importance\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "data=pd.read_csv('data3.csv',header=None)\n",
    "\n",
    "df=data.drop([0,1],0)\n",
    "\n",
    "df[28].replace(0,np.nan) # 학생수 0을 na 로 변환\n",
    "df2=df.dropna(subset=[28]) #전체학생수 na는 제거\n",
    "df3=df2.drop([45],1) \n",
    "#df3.info()\n",
    "df3[df3[28].values!=0]\n",
    "data=df3.iloc[1:, 12:] #데이터 추출\n",
    "\n",
    "data.columns=['일반고','특성화고','과고','외고','특목고합','예체고','마이스터','자율사','자율공','자율형합','기타','총진학','취업','미진학',\n",
    "              '교사1인당주수업','교사1인당학생','전체학생수',\n",
    "              '체험예산지원','1인당체험예산지원','동아리수','동아리참여학생수','동아리1인당참여율','동아리예산지원','1인당동아리예산',\n",
    "              '도서관자료비','도서운영비예산','1인당도서관예산','대출자료수','1인당 대출자료수',\n",
    "             '교과교실제', '자율학교', '이동수업', '영어']\n",
    "\n",
    "data['코드']=df3[3]\n",
    "data['지역']=df3[8]\n",
    "\n",
    "data=data[data.교과교실제!='0'] #na 값 제거\n",
    "data=data[data.자율학교!='0']\n",
    "dt=data.iloc[:, 29:33]\n",
    "data_enc=pd.get_dummies(dt) #one hot encoding\n",
    "dt1=data.iloc[:, :29]\n",
    "dt2=data.iloc[:, 33:]\n",
    "hak=pd.concat([dt1,data_enc,dt2],axis=1)\n",
    "hak.전체학생수.apply(pd.to_numeric)\n",
    "pd.to_numeric(hak['전체학생수'])\n",
    "hak[['일반고','특성화고','과고','외고','특목고합','예체고','마이스터','자율사','자율공','자율형합','기타','총진학','취업','미진학','교사1인당주수업','교사1인당학생','전체학생수',\n",
    "              '체험예산지원','1인당체험예산지원','동아리수','동아리참여학생수','동아리1인당참여율','동아리예산지원','1인당동아리예산',\n",
    "              '도서관자료비','도서운영비예산','1인당도서관예산','대출자료수','1인당 대출자료수']]=hak[['일반고','특성화고','과고','외고','특목고합','예체고','마이스터','자율사','자율공','자율형합','기타','총진학','취업','미진학','교사1인당주수업','교사1인당학생','전체학생수',\n",
    "              '체험예산지원','1인당체험예산지원','동아리수','동아리참여학생수','동아리1인당참여율','동아리예산지원','1인당동아리예산',\n",
    "              '도서관자료비','도서운영비예산','1인당도서관예산','대출자료수','1인당 대출자료수']].astype(float)\n",
    "\n",
    "hak['score']=3*hak['특목고합']+1.5*hak['자율형합']+hak['일반고']+0.8*hak['마이스터']+0.2*hak['특성화고']\n",
    "hak=hak[hak.score!=0] #이상치 제거\n",
    "hak=hak[hak.예체고<=50]#이상치 제거\n",
    "\n",
    "\n",
    "inputs=hak[['특목고합','자율형합','일반고','마이스터','특성화고','예체고']]\n",
    "x=inputs.values #data type (df->array)\n",
    "kmeans = KMeans(n_clusters=4, random_state=0).fit(x)\n",
    "lab=kmeans.labels_\n",
    "df_lab = pd.DataFrame(lab, columns=['label'])\n",
    "\n",
    "hak_re=hak.reset_index(drop=True) #인덱스 재정의 후 통합\n",
    "hak2=pd.concat([hak_re,df_lab],axis=1) \n",
    "\n",
    "hak2=hak2.dropna() \n",
    "\n",
    "\n",
    "targetX=hak2.iloc[:,[14,15,18,21,23,26,28,30,32,34,36]]\n",
    "targetY=hak2.label\n",
    "data_train,data_test,label_train,label_test=train_test_split(targetX,targetY,test_size=.3,random_state=0)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### 데이터 전처리 - 데이터 표준화 작업\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "\n",
    "\n",
    "\n",
    "sc = RobustScaler()\n",
    "x_train_std = sc.fit_transform(data_train)\n",
    "x_test_std = sc.transform(data_test)\n",
    "\n",
    "\n",
    "#log scale\n",
    "#x_train_log=np.log(x_train)\n",
    "\n",
    "\n",
    "\n",
    "#label 값을 카테고리화\n",
    "y_train=label_train.astype('category')\n",
    "y_test=label_test.astype('category')\n",
    "\n",
    "#모형 시각화\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "plt.rc('font',family='Malgun Gothic')\n",
    "\n",
    "\n",
    "\n",
    "# if hak2['score']<=0, hak2['label']=3 을 4로 변화\n",
    "# 조건에 맞게 값 변화\n",
    "#hak2.loc[hak2['score'] <= 30, ['label']] = 4\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# random forest\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier \n",
    "from sklearn.datasets import make_moons\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "model=RandomForestClassifier(n_estimators=10,random_state=8)\n",
    "fittedModel=model.fit(x_train_std,y_train)\n",
    "\n",
    "fittedModel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.score(x_train_std,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.score(x_test_std,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"train set 정확도:\",fittedModel.score(x_train_std,y_train))\n",
    "print(\"test set 정확도:\",fittedModel.score(x_test_std,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#특성 중요도\n",
    "fittedModel.feature_importances_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict=fittedModel.predict(x_test_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accurateRate=accuracy_score(y_test,predict)\n",
    "print('정확도: ',accurateRate)\n",
    "print('오류:',1-accurateRate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix(y_test,predict) \n",
    "# 0:진학 중\n",
    "# 1:취업\n",
    "# 2:진학 상\n",
    "# 3:진학 하\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "depth_set=[3,4,5,6,7,8,9,10]\n",
    "dt_models=[]\n",
    "accuracy_set=[]\n",
    "\n",
    "\n",
    "for depth in depth_set:\n",
    "    model=RandomForestClassifier(n_estimators=15,max_depth=depth, random_state=0)\n",
    "    model.fit(x_train_std,y_train)\n",
    "    accuracy=model.score(x_test_std,y_test)\n",
    "    \n",
    "    dt_models.append(model)\n",
    "    accuracy_set.append(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"train set 정확도:\",fittedModel.score(x_train_std,y_train))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(accuracy_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 파라미터 탐색 결과 가장 좋은 모델과 validation set 에 대한 정확도\n",
    "max_value=max(accuracy_set)\n",
    "max_index=accuracy_set.index(max_value)\n",
    "print(max_index)\n",
    "print(max_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 가장 좋은 모델\n",
    "fittedModel=dt_models[max_index]\n",
    "fittedModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#가장 좋은 모델을 가져와 Test_set 에 대한 예측 성능 평가\n",
    "y_test_hat=dt_models[max_index].predict(x_test_std)\n",
    "print('정확도:',metrics.accuracy_score(y_test,y_test_hat))\n",
    "print(metrics.confusion_matrix(y_test,y_test_hat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = data_train.columns.values\n",
    "print(names)\n",
    "importances = fittedModel.feature_importances_\n",
    "sorted_importances = np.argsort(importances)\n",
    "print(sorted_importances)\n",
    "print(\"변수 중요도\")\n",
    "print(sorted(zip(map(lambda x: round(x, 4), fittedModel.feature_importances_), names), reverse=True))\n",
    "padding = np.arange(len(names)) + 0.5\n",
    "plt.barh(padding, importances[sorted_importances], align='center')\n",
    "# plt.barh(padding, importances[sorted_importances])\n",
    "plt.yticks(padding, names[sorted_importances])\n",
    "plt.xlabel(\"Relative Importance\")\n",
    "plt.title(\"Feature Importance\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#대표변수 3개만"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 데이터 전처리 - 데이터 표준화 작업\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "x1=data_train.iloc[:,[3,6]] #학생변수\n",
    "x1_std=sc.fit_transform(x1)\n",
    "\n",
    "x2=data_train.iloc[:,[0,1]] #교사변수\n",
    "x2_std=sc.fit_transform(x1)\n",
    "\n",
    "x3=data_train.iloc[:,[2,4,5,7,8,9,10]] #학교변수\n",
    "x3_std=sc.fit_transform(x1)\n",
    "\n",
    "x_train_std = sc.fit_transform(data_train)  #변수 전체\n",
    "\n",
    "x4=data_train.iloc[:,[0,5,6]] #3그룹의 대표변수\n",
    "x4_std=sc.fit_transform(x4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#label 값을 카테고리화\n",
    "y_train=y_train.astype('category')\n",
    "y_test=y_test.astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# random forest\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier \n",
    "from sklearn.datasets import make_moons\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "model=RandomForestClassifier(n_estimators=10,random_state=8)\n",
    "#fittedModel=model.fit(x_train_std,y_train)\n",
    "fittedModel=model.fit(x4_std,y_train)\n",
    "fittedModel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xt4=data_test.iloc[:,[0,5,6]] #3그룹의 대표변수\n",
    "xt4_std=sc.fit_transform(xt4)\n",
    "\n",
    "model.score(xt4_std,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = x4.columns.values\n",
    "print(names)\n",
    "importances = fittedModel.feature_importances_\n",
    "sorted_importances = np.argsort(importances)\n",
    "print(sorted_importances)\n",
    "print(\"변수 중요도\")\n",
    "print(sorted(zip(map(lambda x: round(x, 4), fittedModel.feature_importances_), names), reverse=True))\n",
    "padding = np.arange(len(names)) + 0.5\n",
    "plt.barh(padding, importances[sorted_importances], align='center')\n",
    "# plt.barh(padding, importances[sorted_importances])\n",
    "plt.yticks(padding, names[sorted_importances])\n",
    "plt.xlabel(\"Relative Importance\")\n",
    "plt.title(\"Feature Importance\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#특성 중요도\n",
    "fittedModel.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "data=pd.read_csv('data3.csv',header=None)\n",
    "\n",
    "df=data.drop([0,1],0)\n",
    "\n",
    "df[28].replace(0,np.nan) # 학생수 0을 na 로 변환\n",
    "df2=df.dropna(subset=[28]) #전체학생수 na는 제거\n",
    "df3=df2.drop([45],1) \n",
    "#df3.info()\n",
    "df3[df3[28].values!=0]\n",
    "data=df3.iloc[1:, 12:] #데이터 추출\n",
    "\n",
    "data.columns=['일반고','특성화고','과고','외고','특목고합','예체고','마이스터','자율사','자율공','자율형합','기타','총진학','취업'\n",
    "              ,'미진학',\n",
    "              '교사1인당주수업','교사1인당학생','전체학생수',\n",
    "              '체험예산지원','1인당체험예산지원','동아리수','동아리참여학생수','동아리1인당참여율','동아리예산지원','1인당동아리예산',\n",
    "              '도서관자료비','도서운영비예산','1인당도서관예산','대출자료수','1인당 대출자료수',\n",
    "             '교과교실제', '자율학교', '이동수업', '영어']\n",
    "\n",
    "data['코드']=df3[3]\n",
    "data['지역']=df3[8]\n",
    "\n",
    "data=data[data.교과교실제!='0'] #na 값 제거\n",
    "data=data[data.자율학교!='0']\n",
    "dt=data.iloc[:, 29:33]\n",
    "data_enc=pd.get_dummies(dt) #one hot encoding\n",
    "dt1=data.iloc[:, :29]\n",
    "dt2=data.iloc[:, 33:]\n",
    "hak=pd.concat([dt1,data_enc,dt2],axis=1)\n",
    "hak.전체학생수.apply(pd.to_numeric)\n",
    "pd.to_numeric(hak['전체학생수'])\n",
    "hak[['일반고','특성화고','과고','외고','특목고합','예체고','마이스터','자율사','자율공','자율형합','기타','총진학','취업','미진학','교사1인당주수업','교사1인당학생','전체학생수',\n",
    "              '체험예산지원','1인당체험예산지원','동아리수','동아리참여학생수','동아리1인당참여율','동아리예산지원','1인당동아리예산',\n",
    "              '도서관자료비','도서운영비예산','1인당도서관예산','대출자료수','1인당 대출자료수']]=hak[['일반고','특성화고','과고','외고','특목고합','예체고','마이스터','자율사','자율공','자율형합','기타','총진학','취업','미진학','교사1인당주수업','교사1인당학생','전체학생수',\n",
    "              '체험예산지원','1인당체험예산지원','동아리수','동아리참여학생수','동아리1인당참여율','동아리예산지원','1인당동아리예산',\n",
    "              '도서관자료비','도서운영비예산','1인당도서관예산','대출자료수','1인당 대출자료수']].astype(float)\n",
    "\n",
    "hak['score']=3*hak['특목고합']+1.5*hak['자율형합']+hak['일반고']+0.8*hak['마이스터']+0.2*hak['특성화고']\n",
    "hak=hak[hak.score!=0] #이상치 제거\n",
    "hak=hak[hak.예체고<=50]#이상치 제거\n",
    "\n",
    "\n",
    "inputs=hak[['특목고합','자율형합','일반고','마이스터','특성화고','예체고']]\n",
    "x=inputs.values #data type (df->array)\n",
    "kmeans = KMeans(n_clusters=4, random_state=0).fit(x)\n",
    "lab=kmeans.labels_\n",
    "df_lab = pd.DataFrame(lab, columns=['label'])\n",
    "\n",
    "hak_re=hak.reset_index(drop=True) #인덱스 재정의 후 통합\n",
    "hak2=pd.concat([hak_re,df_lab],axis=1) \n",
    "\n",
    "hak2=hak2.dropna() \n",
    "\n",
    "\n",
    "targetX=hak2.iloc[:,[14,15,18,21,23,26,28,30,32,34,36]]\n",
    "targetY=hak2.score  #regression 용도 연속형 변수\n",
    "data_train,data_test,label_train,label_test=train_test_split(targetX,targetY,test_size=.3,random_state=0)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### 데이터 전처리 - 데이터 표준화 작업\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "\n",
    "\n",
    "\n",
    "sc = RobustScaler()\n",
    "x_train_std = sc.fit_transform(data_train)\n",
    "x_test_std = sc.transform(data_test)\n",
    "\n",
    "\n",
    "#log scale\n",
    "#x_train_log=np.log(x_train)\n",
    "\n",
    "\n",
    "\n",
    "#label 값을 카테고리화\n",
    "y_train=label_train.astype('category')\n",
    "y_test=label_test.astype('category')\n",
    "\n",
    "#모형 시각화\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "plt.rc('font',family='Malgun Gothic')\n",
    "\n",
    "\n",
    "\n",
    "# if hak2['score']<=0, hak2['label']=3 을 4로 변화\n",
    "# 조건에 맞게 값 변화\n",
    "#hak2.loc[hak2['score'] <= 30, ['label']] = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lasso\n",
    "\n",
    "import pandas as pd\n",
    "from pandas import Series\n",
    "import numpy as np\n",
    "from sklearn import linear_model as lm\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "np.set_printoptions(linewidth=400)    # screen size\n",
    "np.set_printoptions(threshold=np.inf) # print all numpy output\n",
    "\n",
    "print('[alpha=0]')\n",
    "glm_lasso = lm.Lasso(alpha=0).fit(x_train_std,y_train)\n",
    "predLabelTrain = glm_lasso.predict(x_train_std)\n",
    "r2 = r2_score(y_train,predLabelTrain)\n",
    "print('r-squared = ', r2)\n",
    "mse = mean_squared_error(y_train,predLabelTrain)\n",
    "print('mse = {:,.0f}'.format(mse))\n",
    "lasso_coef =  Series( glm_lasso.coef_,data_train.columns).sort_values()\n",
    "print('type of lasso_coef:', type(lasso_coef))\n",
    "# print(lasso_coef.size)\n",
    "print('beta coefficient of variables')\n",
    "for i in range(lasso_coef.size):\n",
    "\tprint('{:4s} : {:10.8f}'.format(lasso_coef.index.values[i],lasso_coef.values[i]))\n",
    "plt.figure(1)\n",
    "lasso_coef.plot(kind='bar',grid=True)\n",
    "plt.savefig(\"lasso_alpha0.png\")\n",
    "\n",
    "\n",
    "print('[alpha=0.7]')\n",
    "glm_lasso = lm.Lasso(\n",
    "\talpha=0.7\n",
    "\t).fit(x_train_std,y_train)\n",
    "predLabelTrain = glm_lasso.predict(x_train_std)\n",
    "r2 = r2_score(y_train,predLabelTrain)\n",
    "print('r-squared = ', r2)\n",
    "mse = mean_squared_error(y_train,predLabelTrain)\n",
    "print('mse = {:,.0f}'.format(mse))\n",
    "lasso_coef =  Series(\n",
    "\t glm_lasso.coef_\n",
    "\t,data_train.columns\n",
    "\t).sort_values()\n",
    "print('type of lasso_coef:', type(lasso_coef))\n",
    "# print(lasso_coef.size)\n",
    "print('beta coefficient of variables')\n",
    "for i in range(lasso_coef.size):\n",
    "\tprint('{:4s} : {:10.8f}'.format(lasso_coef.index.values[i],lasso_coef.values[i]))\n",
    "plt.figure(2)\n",
    "lasso_coef.plot(kind='bar',grid=True)\n",
    "plt.savefig(\"lasso_alpha002.png\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import linear_model as lm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "np.set_printoptions(linewidth=400)    # screen size\n",
    "np.set_printoptions(threshold=np.inf) # print all numpy output\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "glm_lasso = lm.Lasso(alpha=0).fit(x_train_std,y_train)\n",
    "\n",
    "\n",
    "\n",
    "predLabelTest = glm_lasso.predict(x_test_std)\n",
    "\n",
    "# print(predLabelTest)\n",
    "\n",
    "actualLabelTest=y_test\n",
    "arrLabelTest = np.stack([actualLabelTest, predLabelTest]).T\n",
    "nrow = arrLabelTest.shape[0]\n",
    "for i in range(nrow):\n",
    "\tprint('actual={:10,.0f}  pred={:10,.0f}'.format(arrLabelTest[i,0],arrLabelTest[i,1]))\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "mse = mean_squared_error(actualLabelTest,predLabelTest)\n",
    "print('{:12,.0f}'.format(mse))\n",
    "\n",
    "plt.figure(1)\n",
    "plt.scatter(x_test_std[:,1],actualLabelTest)\n",
    "plt.show() \n",
    "\n",
    "plt.figure(2)\n",
    "plt.scatter(actualLabelTest,actualLabelTest)\n",
    "plt.scatter(actualLabelTest,predLabelTest)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multinomial Logit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as st"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "x = st.add_constant(x_train_std, prepend = False)\n",
    "y=y_train\n",
    "\n",
    "x=data_train\n",
    "x_std=sc.fit_transform(x)\n",
    "\n",
    "x4=x.iloc[:,[0,5,6]] #3그룹의 대표변수\n",
    "x4_std=sc.fit_transform(x4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "mdl3 = st.MNLogit(y,x)\n",
    "mdl4 = st.MNLogit(y,x4)\n",
    "\n",
    "mdl = st.MNLogit(y,x_std)#전체변수\n",
    "mdl2 = st.MNLogit(y,x4_std)#대표변수 3개"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mdl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mdl_fit = mdl.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mdl_fit.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### marginal effects ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mdl_margeff = mdl_fit.get_margeff()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mdl_margeff.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_std.columns=[\n",
    "#               '교사1인당주수업','교사1인당학생',\n",
    "#              '1인당체험예산지원','동아리1인당참여율','1인당동아리예산',\n",
    "#               '1인당도서관예산','1인당 대출자료수',\n",
    "#              '교과교실제', '자율학교', '이동수업', '영어']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x1='교사1인당주수업'\n",
    "# x2='교사1인당학생'\n",
    "# x3='1인당체험예산지원'\n",
    "# x4='동아리1인당참여율'\n",
    "# x5='1인당동아리예산'\n",
    "# x6='1인당도서관예산'\n",
    "# x7='1인당 대출자료수'\n",
    "# x8='교과교실제'\n",
    "# x9='자율학교'\n",
    "# x10='이동수업'\n",
    "# x11='영어'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학교와 지역 관계"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=hak2.sort_values('score',ascending=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['지역'].head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['지역'].tail(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top=hak2.sort_values(by=['score'], axis=0, ascending=False).head(50)\n",
    "top.지역.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tail=hak2.sort_values(by=['score'], axis=0, ascending=False).tail(50)\n",
    "\n",
    "tail.지역.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#svm\n",
    "#LinearSVC\n",
    "\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn import svm    # To fit the svm classifier\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X = x_train_std[:, 5:7]  # we only take the two features.\n",
    "# y = y_train\n",
    "# C = 1.0  # SVM regularization parameter\n",
    " \n",
    "# # SVC with linear kernel\n",
    "# svc = svm.SVC(kernel='linear', C=C).fit(X, y)\n",
    "# # LinearSVC (linear kernel)\n",
    "# lin_svc = svm.LinearSVC(C=C).fit(X, y)\n",
    "# # SVC with RBF kernel\n",
    "# rbf_svc = svm.SVC(kernel='rbf', gamma=0.5, C=C).fit(X, y)\n",
    "# # SVC with polynomial (degree 3) kernel\n",
    "# poly_svc = svm.SVC(kernel='poly', degree=3, C=C).fit(X, y)\n",
    "\n",
    "# h = .02  # step size in the mesh\n",
    " \n",
    "# # create a mesh to plot in\n",
    "# x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "# y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "# xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "# \t                     np.arange(y_min, y_max, h))\n",
    "# # title for the plots\n",
    "# titles = ['SVC with linear kernel',\n",
    "# \t   'LinearSVC (linear kernel)',\n",
    "# \t    'SVC with RBF kernel',\n",
    "# \t    'SVC with polynomial (degree 3) kernel']\n",
    " \n",
    " \n",
    "# for i, clf in enumerate((svc, lin_svc, rbf_svc, poly_svc)):\n",
    "# \t # Plot the decision boundary. For that, we will assign a color to each\n",
    "# \t # point in the mesh [x_min, x_max]x[y_min, y_max].\n",
    "# \t plt.subplot(2, 2, i + 1)\n",
    "# \t plt.subplots_adjust(wspace=0.4, hspace=0.4)\n",
    " \n",
    "# \t Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    " \n",
    "# \t # Put the result into a color plot\n",
    "# \t Z = Z.reshape(xx.shape)\n",
    "# \t plt.contourf(xx, yy, Z, cmap=plt.cm.coolwarm, alpha=0.8)\n",
    " \n",
    "# \t # Plot also the training points\n",
    "# \t plt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.coolwarm)\n",
    "# \t plt.xlabel('1인당 도서관 예산')\n",
    "# \t plt.ylabel('1인당 대출자료수')\n",
    "# \t plt.xlim(xx.min(), xx.max())\n",
    "# \t plt.ylim(yy.min(), yy.max())\n",
    "# \t plt.xticks(())\n",
    "# \t plt.yticks(())\n",
    "# \t plt.title(titles[i])\n",
    " \n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X = x_train_std[:, 3:5]  # we only take the two features.\n",
    "# y = y_train\n",
    "# C = 1.0  # SVM regularization parameter\n",
    " \n",
    "# # SVC with linear kernel\n",
    "# svc = svm.SVC(kernel='linear', C=C).fit(X, y)\n",
    "# # LinearSVC (linear kernel)\n",
    "# lin_svc = svm.LinearSVC(C=C).fit(X, y)\n",
    "# # SVC with RBF kernel\n",
    "# rbf_svc = svm.SVC(kernel='rbf', gamma=0.5, C=C).fit(X, y)\n",
    "# # SVC with polynomial (degree 3) kernel\n",
    "# poly_svc = svm.SVC(kernel='poly', degree=3, C=C).fit(X, y)\n",
    "\n",
    "# h = .02  # step size in the mesh\n",
    " \n",
    "# # create a mesh to plot in\n",
    "# x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "# y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "# xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "# \t                     np.arange(y_min, y_max, h))\n",
    "# # title for the plots\n",
    "# titles = ['SVC with linear kernel',\n",
    "# \t   'LinearSVC (linear kernel)',\n",
    "# \t    'SVC with RBF kernel',\n",
    "# \t    'SVC with polynomial (degree 3) kernel']\n",
    " \n",
    " \n",
    "# for i, clf in enumerate((svc, lin_svc, rbf_svc, poly_svc)):\n",
    "# \t # Plot the decision boundary. For that, we will assign a color to each\n",
    "# \t # point in the mesh [x_min, x_max]x[y_min, y_max].\n",
    "# \t plt.subplot(2, 2, i + 1)\n",
    "# \t plt.subplots_adjust(wspace=0.4, hspace=0.4)\n",
    " \n",
    "# \t Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    " \n",
    "# \t # Put the result into a color plot\n",
    "# \t Z = Z.reshape(xx.shape)\n",
    "# \t plt.contourf(xx, yy, Z, cmap=plt.cm.coolwarm, alpha=0.1)\n",
    " \n",
    "# \t # Plot also the training points\n",
    "# \t plt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.coolwarm)\n",
    "# \t plt.xlabel('동아리 1인당 참여율')\n",
    "# \t plt.ylabel('1인당 동아리 예산')\n",
    "# \t plt.xlim(xx.min(), xx.max())\n",
    "# \t plt.ylim(yy.min(), yy.max())\n",
    "# \t plt.xticks(())\n",
    "# \t plt.yticks(())\n",
    "# \t plt.title(titles[i])\n",
    " \n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tensorflow 로 비선형 SVM 구현\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.framework import ops\n",
    "ops.reset_default_graph()\n",
    "\n",
    "sess = tf.Session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#데이터에서 독립변수 ( 교사당 주수업 과 1인당 도서관 예산) 와 종속변수(진학등급) 를 데이터를 변수에 대입한다.\n",
    "x_vals = np.array([[x[0], x[5]] for x in x_train_std])\n",
    "y_vals = np.array([1 if y==2 else -1 for y in y_train])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 상급학교 의 교사당 주수업 ==> class1_x \n",
    "# 상급학교 의 1인당 도서관 예산 ==> class1_y\n",
    "# 그외학교 의 교사당 주수업==> class2_x \n",
    "# 그외학교 의 1인당 도서관 예산 ==> class2_y\n",
    "class1_x = [x[0] for i,x in enumerate(x_vals) if y_vals[i]==1]\n",
    "class1_y = [x[1] for i,x in enumerate(x_vals) if y_vals[i]==1]\n",
    "class2_x = [x[0] for i,x in enumerate(x_vals) if y_vals[i]==-1]\n",
    "class2_y = [x[1] for i,x in enumerate(x_vals) if y_vals[i]==-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 일괄 작업 크기를 선언 \n",
    "batch_size = 150\n",
    "\n",
    "# placeholders 초기화 \n",
    "x_data = tf.placeholder(shape=[None, 2], dtype=tf.float32)\n",
    "y_target = tf.placeholder(shape=[None, 1], dtype=tf.float32)\n",
    "prediction_grid = tf.placeholder(shape=[None, 2], dtype=tf.float32)\n",
    "\n",
    "#  모델 변수 b 를 선언 \n",
    "b = tf.Variable(tf.random_normal(shape=[1,batch_size]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#가우시안 커널(가중치)을 선언한다. gamma 값이 어떤 영향을 미치는지알아보자.\n",
    "\n",
    "gamma = tf.constant(-7.0) \n",
    "# gamma 값의 변화를 보자\n",
    "sq_vec = tf.multiply(2., tf.matmul(x_data, tf.transpose(x_data)))\n",
    "my_kernel = tf.exp(tf.multiply(gamma, tf.abs(sq_vec)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss 함수 \n",
    "\n",
    "# svn model\n",
    "first_term = tf.reduce_sum(b)\n",
    "b_vec_cross = tf.matmul(tf.transpose(b), b)\n",
    "y_target_cross = tf.matmul(y_target, tf.transpose(y_target))\n",
    "second_term = tf.reduce_sum(tf.multiply(my_kernel, tf.multiply(b_vec_cross, y_target_cross)))\n",
    "loss = tf.negative(tf.subtract(first_term, second_term))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#svm을 통해 예측을 수행하기 위해 예측 커널 함수를 만든다.\n",
    "\n",
    "# Gaussian (RBF) prediction kernel\n",
    "rA = tf.reshape(tf.reduce_sum(tf.square(x_data), 1),[-1,1])\n",
    "rB = tf.reshape(tf.reduce_sum(tf.square(prediction_grid), 1),[-1,1])\n",
    "pred_sq_dist = tf.add(tf.subtract(rA, tf.multiply(2., tf.matmul(x_data, tf.transpose(prediction_grid)))), tf.transpose(rB))\n",
    "pred_kernel = tf.exp(tf.multiply(gamma, tf.abs(pred_sq_dist)))\n",
    "\n",
    "prediction_output = tf.matmul(tf.multiply(tf.transpose(y_target),b), pred_kernel)\n",
    "prediction = tf.sign(prediction_output-tf.reduce_mean(prediction_output))\n",
    "accuracy = tf.reduce_mean(tf.cast(tf.equal(tf.squeeze(prediction), tf.squeeze(y_target)), tf.float32))\n",
    "\n",
    "\n",
    "my_opt = tf.train.GradientDescentOptimizer(0.01)\n",
    "train_step = my_opt.minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 변수 초기화 \n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)\n",
    "\n",
    "\n",
    "# 루프를 반복하면서 loss 함수 값과 일괄 작업 대상  , 데이터 정확도를 기록해둔다. \n",
    "\n",
    "# Training loop\n",
    "loss_vec = []\n",
    "batch_accuracy = []\n",
    "for i in range(300):\n",
    "    rand_index = np.random.choice(len(x_vals), size=batch_size)\n",
    "    rand_x = x_vals[rand_index]\n",
    "    rand_y = np.transpose([y_vals[rand_index]])\n",
    "    sess.run(train_step, feed_dict={x_data: rand_x, y_target: rand_y})\n",
    "    \n",
    "    temp_loss = sess.run(loss, feed_dict={x_data: rand_x, y_target: rand_y})\n",
    "    loss_vec.append(temp_loss)\n",
    "    \n",
    "    acc_temp = sess.run(accuracy, feed_dict={x_data: rand_x,\n",
    "                                             y_target: rand_y,\n",
    "                                             prediction_grid:rand_x})\n",
    "    batch_accuracy.append(acc_temp)\n",
    "    \n",
    "    # 데이터가 많아서 일부만 출력해보자. \n",
    "    if (i+1)%50==0:\n",
    "        print('Step #' + str(i+1))\n",
    "        print('Loss = ' + str(temp_loss))\n",
    "\n",
    "\n",
    "\n",
    "#출력:\n",
    "x_min, x_max = x_vals[:, 0].min() - 1, x_vals[:, 0].max() + 1\n",
    "y_min, y_max = x_vals[:, 1].min() - 1, x_vals[:, 1].max() + 1\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.02),\n",
    "                     np.arange(y_min, y_max, 0.02))\n",
    "grid_points = np.c_[xx.ravel(), yy.ravel()]\n",
    "[grid_predictions] = sess.run(prediction, feed_dict={x_data: rand_x,\n",
    "                                                   y_target: rand_y,\n",
    "                                                   prediction_grid: grid_points})\n",
    "grid_predictions = grid_predictions.reshape(xx.shape)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "plt.contourf(xx, yy, grid_predictions, cmap=plt.cm.Paired, alpha=0.8)\n",
    "plt.plot(class1_x, class1_y, 'ko', label='취업 위주 학교')\n",
    "plt.plot(class2_x, class2_y, 'rx', label='그외 학교')\n",
    "plt.title('Gaussian SVM Results on Data')\n",
    "plt.xlabel('교사당 주수업 일수')\n",
    "plt.ylabel('1인당 도서관 예산')\n",
    "plt.legend(loc='upper right')\n",
    "# plt.ylim([-0.5, 3.0])\n",
    "# plt.xlim([3.5, 8.5])\n",
    "plt.show()\n",
    "\n",
    "\n",
    "plt.plot(batch_accuracy, 'k-', label='Accuracy')\n",
    "plt.title('Batch Accuracy')\n",
    "plt.xlabel('Generation')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend(loc='upper right')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "plt.plot(loss_vec, 'k-')\n",
    "plt.title('Loss per Generation')\n",
    "plt.xlabel('Generation')\n",
    "plt.ylabel('Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
